{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "E7GLx5EHjlph"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.preprocessing import normalize, MinMaxScaler\n",
        "\n",
        "np.random.seed(132)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUOtinifjvfd",
        "outputId": "3e45bb9a-508b-430a-d0f5-4f9697c9ac63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jQngAX3kjlpl"
      },
      "outputs": [],
      "source": [
        "labels = pd.read_csv('/content/drive/MyDrive/Colab projects/Neural_Network_From_Scratch/data/Labels.csv')\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab projects/Neural_Network_From_Scratch/data/Data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mnr4uc9bjlpq"
      },
      "outputs": [],
      "source": [
        "labels = labels.drop(columns=['Unnamed: 0'])\n",
        "data = data.drop(columns=['Unnamed: 0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUaI3Vr1kX_f",
        "outputId": "230e061d-3441-489e-8122-d26bbebfbf31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13611, 16)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train = data.to_numpy()\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uvrJI829jlps"
      },
      "outputs": [],
      "source": [
        "enc = OneHotEncoder()\n",
        "ohe_labels = enc.fit_transform(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LOi2UvKjlpt",
        "outputId": "4ad05896-32c9-4a63-efad-f8b36e1ef8a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13611, 7)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train = ohe_labels.toarray()\n",
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRY9m8_1q1jI",
        "outputId": "b4308a75-7fa2-4817-d9d9-91a37cf6e7d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0.]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7ruuHQ-jlpv",
        "outputId": "edfe2cc2-0164-4e92-a815-6daf05935973"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[2.83950000e+04, 6.10291000e+02, 2.08178117e+02, 1.73888747e+02,\n",
              "        1.19719142e+00, 5.49812187e-01, 2.87150000e+04, 1.90141097e+02,\n",
              "        7.63922518e-01, 9.88855999e-01, 9.58027126e-01, 9.13357755e-01,\n",
              "        7.33150600e-03, 3.14728900e-03, 8.34222388e-01, 9.98723889e-01],\n",
              "       [2.87340000e+04, 6.38018000e+02, 2.00524796e+02, 1.82734419e+02,\n",
              "        1.09735646e+00, 4.11785251e-01, 2.91720000e+04, 1.91272751e+02,\n",
              "        7.83968133e-01, 9.84985603e-01, 8.87033637e-01, 9.53860842e-01,\n",
              "        6.97865900e-03, 3.56362400e-03, 9.09850506e-01, 9.98430331e-01],\n",
              "       [2.93800000e+04, 6.24110000e+02, 2.12826130e+02, 1.75931143e+02,\n",
              "        1.20971266e+00, 5.62727317e-01, 2.96900000e+04, 1.93410904e+02,\n",
              "        7.78113248e-01, 9.89558774e-01, 9.47849473e-01, 9.08774239e-01,\n",
              "        7.24391200e-03, 3.04773300e-03, 8.25870617e-01, 9.99066137e-01],\n",
              "       [3.00080000e+04, 6.45884000e+02, 2.10557999e+02, 1.82516516e+02,\n",
              "        1.15363806e+00, 4.98615976e-01, 3.07240000e+04, 1.95467062e+02,\n",
              "        7.82681273e-01, 9.76695743e-01, 9.03936374e-01, 9.28328835e-01,\n",
              "        7.01672900e-03, 3.21456200e-03, 8.61794425e-01, 9.94198849e-01],\n",
              "       [3.01400000e+04, 6.20134000e+02, 2.01847882e+02, 1.90279279e+02,\n",
              "        1.06079802e+00, 3.33679658e-01, 3.04170000e+04, 1.95896503e+02,\n",
              "        7.73098035e-01, 9.90893250e-01, 9.84877069e-01, 9.70515523e-01,\n",
              "        6.69701000e-03, 3.66497200e-03, 9.41900381e-01, 9.99166059e-01]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke2y6XRxoTx_"
      },
      "source": [
        "### Normalizing values since they are very large\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "xqwp0sL3oSBq"
      },
      "outputs": [],
      "source": [
        "X_train_norm = normalize(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "Hs5cxYk3IEjd"
      },
      "outputs": [],
      "source": [
        "mms = MinMaxScaler()\n",
        "X_train_mms = mms.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "GaaCeiq4jlpx"
      },
      "outputs": [],
      "source": [
        "class ActivationFunction():\n",
        "    def __init__(self):\n",
        "        self.name = self.__class__.__name__\n",
        "\n",
        "class Sigmoid(ActivationFunction):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, X):\n",
        "      return 1.0/(1.0+np.exp(-X))\n",
        "\n",
        "    def backward(self, X):\n",
        "      val = self.forward(X)\n",
        "      return val*(1-val)\n",
        "\n",
        "class Softmax(ActivationFunction):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(f'input : {x}')\n",
        "        #input is a matrix. Return element wise softmax\n",
        "        #f = np.exp(x - np.max(x))  # shift values\n",
        "        #print(f'output : {np.exp(x)/(np.sum(np.exp(x)))}')\n",
        "        #return f / f.sum(axis=0)\n",
        "        return np.exp(x)/(np.sum(np.exp(x)))\n",
        "\n",
        "    def backward(self, x):\n",
        "        softmax = self.forward(x)\n",
        "        return softmax*(1-softmax)\n",
        "\n",
        "class ReLU(ActivationFunction):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self,X):\n",
        "        return X * (X > 0)\n",
        "\n",
        "    def backward(self,X):\n",
        "        X[X <= 0.0] = 0.0\n",
        "        X[X > 0.0] = 1.0\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "JM1Ft6Ckjlp0"
      },
      "outputs": [],
      "source": [
        "class LossFunction():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.name = self.__class__.__name__\n",
        "\n",
        "class CrossEntropyLoss(LossFunction):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def computeLoss(self, Y_true, Y_pred):\n",
        "      for p in Y_pred[0]:\n",
        "        if np.isnan(p) or p<10e-8:\n",
        "          p=10e-8\n",
        "      loss=np.multiply(Y_pred,Y_true)\n",
        "      loss=loss[loss!=0]\n",
        "      loss=-np.log(loss)\n",
        "      loss=np.mean(loss)\n",
        "\n",
        "      return loss\n",
        "\n",
        "    def backward(self, Y_pred,Y_true):\n",
        "      return -Y_true/(Y_pred)\n",
        "\n",
        "    def last_output_derivative(self, Y_pred,Y_true):\n",
        "      for p in Y_pred[0]:\n",
        "        if np.isnan(p) or p<10e-8:\n",
        "          p=10e-8\n",
        "      return -(Y_true - Y_pred)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "qHUYd6u8DhAG"
      },
      "outputs": [],
      "source": [
        "def computeAccuracy(y_true, y_pred):\n",
        "  true_count = 0\n",
        "  total = len(y_pred)\n",
        "  for i in range(len(y_pred)):\n",
        "    #print(f'True {np.argmax(y_true)} pred {np.argmax(y_pred)}')\n",
        "    if (np.argmax(y_true[i]) == np.argmax(y_pred[i])):\n",
        "      true_count += 1\n",
        "\n",
        "  return true_count/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "gSln57YFjlp5"
      },
      "outputs": [],
      "source": [
        "class NeuralNetworkClassifier():\n",
        "\n",
        "    def __init__(self, layer_weights=[32,64], lr = 0.000005, activation_fn = ReLU(), output_fn = Softmax(), loss_fn=CrossEntropyLoss()):\n",
        "        self.layers = layer_weights\n",
        "        self.eta = lr\n",
        "        self.output_layer = None\n",
        "        self.input_layer = None\n",
        "        self.loss_fn = loss_fn\n",
        "        self.output_fn = output_fn\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "        #Weights and biases are to be stored as KV pairs\n",
        "        self.W = {}\n",
        "        self.B = {}\n",
        "\n",
        "        #Intermediate computations to be stored as KV pairs\n",
        "        self.A = {}\n",
        "        self.H = {}\n",
        "\n",
        "        #Gradients to be stored for weights, biases and intermediate layers\n",
        "        self.dW = {}\n",
        "        self.dB = {}\n",
        "        self.dH = {}\n",
        "        self.dA = {}\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X, Y, epochs=10):\n",
        "        '''\n",
        "        Mandatory call for fitting training data on the model.\n",
        "        Required step since input and output layer size are unknown.\n",
        "        '''\n",
        "\n",
        "        self.input_layer = X.shape[1] #num features\n",
        "        self.output_layer = Y.shape[1] #num classes\n",
        "\n",
        "        self.layers = [self.input_layer] + self.layers + [self.output_layer]\n",
        "\n",
        "        print(f\"Neural network layer sizes : {self.layers}\")\n",
        "\n",
        "        #initialize weights for the layers\n",
        "        self.initialize_weights()\n",
        "\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.dW = {}\n",
        "            self.dB = {}\n",
        "            self.dH = {}\n",
        "            self.dA = {}\n",
        "\n",
        "\n",
        "            #Call forward propagation to train model\n",
        "            preds = []\n",
        "            iteration = 0\n",
        "            for x,y in zip(X,Y):\n",
        "                self.forward_propagation(x)\n",
        "                preds.append(self.H[(len(self.layers)-1)])\n",
        "\n",
        "                if iteration==0:\n",
        "                    for layer_num in range(1,len(self.layers)):\n",
        "                        self.dW[layer_num] = np.zeros((self.layers[layer_num-1], self.layers[layer_num]))\n",
        "\n",
        "                        self.dB[layer_num] = np.zeros((1,self.layers[layer_num]))\n",
        "\n",
        "                        self.dH[layer_num] = np.zeros((self.H[layer_num].shape[0], self.H[layer_num].shape[1]))\n",
        "\n",
        "                        self.dA[layer_num] = np.zeros((self.A[layer_num].shape[0], self.A[layer_num].shape[1]))\n",
        "                iteration +=1\n",
        "                #Perform backpropagation over losses\n",
        "                self.back_propagation(np.array(y), np.array(self.H[(len(self.layers)-1)]).squeeze())\n",
        "\n",
        "            preds = np.array(preds).squeeze()\n",
        "            #Compute loss here to check performance\n",
        "            epoch_loss = self.loss_fn.computeLoss(np.array(Y), np.array(preds))\n",
        "            accuracy = computeAccuracy(Y, preds)\n",
        "\n",
        "            print(f\"Epoch {epoch} :: Loss {epoch_loss} :: Accuracy {accuracy}\")\n",
        "            self.update_weights()\n",
        "\n",
        "        return preds\n",
        "\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        '''\n",
        "        Initialize the weights and biases once the layer sizes are known\n",
        "        '''\n",
        "        for layer_num in range(1,len(self.layers)):\n",
        "            self.W[layer_num] = np.random.randn(self.layers[layer_num-1], self.layers[layer_num])\n",
        "            print(f\"Shape of W{layer_num} = {self.W[layer_num].shape }\")\n",
        "            self.B[layer_num] = np.random.randn(self.layers[layer_num])\n",
        "            print(f\"Shape of B{layer_num} = {self.B[layer_num].shape }\")\n",
        "\n",
        "\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "\n",
        "        #x is a single datapoint (num_feats, )\n",
        "\n",
        "        self.H = {}\n",
        "        self.A = {}\n",
        "        #Check if num features are accurate\n",
        "        if X.shape[0] != self.input_layer:\n",
        "            print(f\"Invalid shape. {X.shape[1]} does not match {self.input_layer}\")\n",
        "            return\n",
        "\n",
        "        #reshaped to have 1 row and all elements as column values (hence -1)\n",
        "        self.H[0] = X.reshape(1,-1).astype(float)\n",
        "\n",
        "        #Repeat from layer 1 till layer l-1 (for final layer the output activation will change, so that will be done separately)\n",
        "        #For [i/p, l1, l2, o/p] sequence of layers, this loop runs for l1 and l2\n",
        "        for layer in range(1, len(self.layers)-1):\n",
        "\n",
        "            self.A[layer] = np.matmul(self.H[(layer-1)],self.W[layer]) + self.B[(layer)]\n",
        "            self.H[layer] = self.activation_fn.forward(self.A[layer]) #compute sigmoid or orher activation here\n",
        "\n",
        "        self.A[(len(self.layers)-1)] = np.matmul(self.H[(len(self.layers)-2)],self.W[(len(self.layers)-1)]) + self.B[(len(self.layers)-1)]\n",
        "        self.H[(len(self.layers)-1)] = self.output_fn.forward(self.A[(len(self.layers)-1)])\n",
        "\n",
        "\n",
        "        return\n",
        "\n",
        "    def back_propagation(self, y_true, y_pred):\n",
        "\n",
        "        dW_i = {}\n",
        "        dB_i = {}\n",
        "        #1. dloss/doutput (h_last_layer)\n",
        "\n",
        "        self.dA[len(self.layers)-1] = self.loss_fn.last_output_derivative(self.H[len(self.layers)-1], y_true)\n",
        "\n",
        "        for layer in range(len(self.layers) -2, 0, -1):\n",
        "\n",
        "            dB_i[layer+1] = self.dA[layer+1]\n",
        "            dW_i[layer+1] = np.matmul(self.H[layer].T,self.dA[layer+1]) #The dimensions of W and DW must match, hence H stays on the left\n",
        "\n",
        "            self.dH[layer] = np.matmul(self.dA[layer+1],self.W[layer+1].T)\n",
        "            self.dA[layer] = np.multiply(self.activation_fn.backward(self.A[layer]),self.dH[layer]) #Hadamard product (element wise multiplication)\n",
        "\n",
        "\n",
        "        dW_i[1] = np.matmul(self.H[0].T,self.dA[1]) #The dimensions of W and DW must match, hence H stays on the left\n",
        "        dB_i[1] = self.dA[1]\n",
        "\n",
        "        for layer in range(1, len(self.layers)):\n",
        "            self.dW[layer] += dW_i[layer]\n",
        "            self.dB[layer] += dB_i[layer]\n",
        "\n",
        "\n",
        "    def update_weights(self):\n",
        "\n",
        "        for layer in range(1, len(self.layers)):\n",
        "            self.W[layer] = self.W[layer]- self.eta*self.dW[layer]\n",
        "            self.B[layer] = self.B[layer]- self.eta*self.dB[layer]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEK4yDYrjlp8",
        "outputId": "48b3030c-9c81-46f1-f396-466ad05e0b8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neural network layer sizes : [16, 32, 64, 7]\n",
            "Shape of W1 = (16, 32)\n",
            "Shape of B1 = (32,)\n",
            "Shape of W2 = (32, 64)\n",
            "Shape of B2 = (64,)\n",
            "Shape of W3 = (64, 7)\n",
            "Shape of B3 = (7,)\n",
            "Epoch 0 :: Loss 27.45505579455285 :: Accuracy 0.19366688707662919\n",
            "Epoch 1 :: Loss 35.974693416576635 :: Accuracy 0.1416501359194769\n",
            "Epoch 2 :: Loss 52.69086195539561 :: Accuracy 0.2605245757108221\n",
            "Epoch 3 :: Loss 34.21109863014995 :: Accuracy 0.09712732348835501\n",
            "Epoch 4 :: Loss 13.277097716851223 :: Accuracy 0.11975607964146646\n",
            "Epoch 5 :: Loss 10.800464257703087 :: Accuracy 0.148923664682977\n",
            "Epoch 6 :: Loss 7.90808288679784 :: Accuracy 0.2605245757108221\n",
            "Epoch 7 :: Loss 5.141597560297759 :: Accuracy 0.1416501359194769\n",
            "Epoch 8 :: Loss 4.087360841719096 :: Accuracy 0.148923664682977\n",
            "Epoch 9 :: Loss 4.422611098562959 :: Accuracy 0.2605245757108221\n",
            "Epoch 10 :: Loss 3.245622861770646 :: Accuracy 0.19366688707662919\n",
            "Epoch 11 :: Loss 3.547900471644359 :: Accuracy 0.2605245757108221\n",
            "Epoch 12 :: Loss 2.434496249439777 :: Accuracy 0.09712732348835501\n",
            "Epoch 13 :: Loss 2.2678136133002758 :: Accuracy 0.2605245757108221\n",
            "Epoch 14 :: Loss 2.1592819310001024 :: Accuracy 0.19366688707662919\n",
            "Epoch 15 :: Loss 2.072707035498514 :: Accuracy 0.2605245757108221\n",
            "Epoch 16 :: Loss 2.126911812324487 :: Accuracy 0.19366688707662919\n",
            "Epoch 17 :: Loss 1.962132765266501 :: Accuracy 0.2605245757108221\n",
            "Epoch 18 :: Loss 1.975620735252495 :: Accuracy 0.19366688707662919\n",
            "Epoch 19 :: Loss 1.9083419577802956 :: Accuracy 0.2605245757108221\n"
          ]
        }
      ],
      "source": [
        "nn = NeuralNetworkClassifier([32, 64])\n",
        "sample_preds = nn.fit(X_train_norm, y_train, epochs=20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "siddesh",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
